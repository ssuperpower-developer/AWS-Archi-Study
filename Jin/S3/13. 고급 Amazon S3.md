
## 다른 스토리지 클래스 간에 object를 옮기는 방법 
- object를 이전할 수 있음
- 예시
		- 그럼 실제로 객체에 자주 액세스하지 않을 걸 알고 있다면 Standard IA로 옮길 수 있고요
	- 만일 객체를 아카이브화 하려는 걸 알고 있다면 Glacier 티어나 Deep Archive 티어로 이전할 수 있어요
> 객체를 수작업으로 옮길 수도 있지만 라이프사이클 규칙을 이용해서 자동으로 옮길 수 있음 


## Lifecycle Rules
- 1. Transition Actions
	- 다른 스토리지 클래스로 이전하기 위해 객체를 설정하는 행위
	- 예시
		- 예를 들어, 생성된 지 60일 후에 Standard 클래스로 이전한다.
		- 또는 6개월 후에 Glacier로 이전해서 아카이브화 한다고 할 수도 있고요
		- 또 Expiration actions도 설정해서 일정한 시간 뒤에 만료되어서 객체를 삭제하도록 설정할 수도 있어요
		- 예를 들어 여러분의 액세스 로그 파일은 365일 후에 삭제할 수 있어요, 또는 버저닝을 활성화했다면 Expiration actions를 사용해서 모든 파일 버전을 삭제할 수도 있죠
		- 또는 이걸 사용해서 불완전한 멀티파트 업로드를 삭제할 수도 있어요
		- 가령 멀티파트 업로드가 2주 이상 된 경우에도 완전히 완료되지 않은 경우에는 그걸 삭제할 수도 있어요
	- rule은 특정한 접두어에 대해 지정할 수도 있음.
		- rule을 버킷 전체에 적용하거나 버킷 안의 특정한 경로에 적용할 수 있죠 
		- 특정한 객체 태그에 대해 규칙을 지정할 수도 있어요 
			- 재무 부서에만 규칙을 적용할 수도 있을 거예요
	- 예시 시나리오 1
		- EC2에 애플리케이션이 있고요, 
		- 그 앱은 프로필 사진이 Amazon S3에 업로드된 후에 이미지 섬네일을 생성해요, 
		- 하지만 그런 섬네일들은 원본 사진으로부터 쉽게 재생성할 수 있어요
		- 그리고 60일 동안만 보관해야 하죠. 
		- 하지만 원본 이미지는 60일 동안은 곧바로 받을 수 있어야 하고요. 
		- 그 후에 사용자는 최장 6시간 동안 기다릴 수 있어요. 
	- 예시 시나리오 1 답변
		- S3 원본 이미지는 Standard 클래스에 있을 수 있고요, 
		- 60일 후에 Glacier로 이전하기 위한 라이프사이클 설정이 있어요
		- 섬네일은 One-Zone IA에 있을 수 있어요, 왜냐면 빈번히 액세스하는 게 아니고 쉽게 재생성할 수 있으니까요
		- 그리고 60일 후에 그것들을 만료시키거나 삭제하는 라이프사이클 설정이 있을 수 있죠
	- 예시 시나리오 2 
		- 여러분의 회사 규칙에 따라 30일 동안은 삭제된 S3 객체를 즉각적으로 복구할 수 있어야 해요. 
		- 그 기간이 지나면 최장 365일 동안은 삭제된 객체를 48시간 이내에 복구할 수 있어야 해요
	- 예시 시나리오 2 답변
		- 우린 S3 버저닝을 활성화해서 객체 버전을 보관할 수 있고요
		- 삭제된 객체들은 실제로 “삭제 마커”에 의해 감춰져 있죠, 그런 다음에 복구할 수 있을 거예요
		- 다음으로 여러분은 현재 버전이 아닌, 즉 최상위 버전이 아닌 객체들을 Standard IA로 이전하기 위한 규칙을 만들 거예요, 즉 그 현재 버전이 아닌 버전들을 아카이브화를 목적으로 Glacier Deep Archive로 이전할 수 있어요

##  Amazon S3 Analytics
>  어떻게 객체를 다른 스토리지 클래스로 이전할 최적의 기간을 결정함???

- Standard나 Standard IA에 관한 추천사항을 제시할 거고요
	- One-Zone IA나 Glacier와는 호환되지 않아요
- S3 버킷이 있고 추가로 S3 Analytics가 실행되고 있는 형태
- .csv 보고서를 생성하고 추천사항과 통계를 제공할 거예요
- 보고서는 매일 업데이트
	- 데이터 분석 결과는 24시간~48시간이 걸릴 수 있어요
- 이런 보고서는 합리적인 라이프사이클 규칙들을 개선해 나갈 수 있다~ 


- 실습 
	- managment - Create lifecycle rule

## 수명 주기 규칙 작업 lifecycle rule

- [x] 스토리지 클래스 간에 객체의 현재 버전 전환
	- 이 작업을 수행하면 현재 버전이 이동합니다.
- [ ] 스토리지 클래스 간에 객체의 이전 버전 전환
	- 이 작업을 수행하면 현재 버전이 아닌 버전이 이동합니다.
- [ ]  객체의 현재 버전 만료
- [ ] 객체의 이전 버전 영구 삭제
	- 만료된 객체 삭제 마커 또는 완료되지 않은 멀티파트 업로드 삭제객체 태그 또는 객체 크기를 기준으로 필터링할 때는 이러한 작업이 지원되지 않습니다.


> 상황에 대해서 어떤 클래스를 쓸 것인지 알아야할 것 같음. 
> ![[Pasted image 20250504213032.png]]


## ==S3 requester pays==
- 버킷 소유자가 이용료를 다 냄.
	- 네트워크 비용도 냄. ㅜㅜㅜ 
> S3 requester pays= 요청자가 데이터 다운로드 비용을 내게 하는 것! 
- 요청자가 aws에서 인증을 받아야함. 익명이면 안됨.


## S3 event notifications
- event = 객체 생성, 제거, 저장, 복제 등... 
- jpg 확장자에만 반응하도록 설정할 수도 있음 
- sns 토픽, 람다 함수 등이 대상이다. 
- 원하는 만큼 S3 event를 만들 수 있음. 
- IAM 권한이 있어야함. 
- s3가 이벤트를 받음. -> SNS  / sqs / rambda
- 리소스 액세스 정책을 만들고 데이터를 전송하도록 허가를 받는다. 
- s3가 우리가 함수를 호출할 권한을 제공해야함. 
- 실습에서는 IAM 안쓰고 SNS  / sqs / rambda  리소스 엑세스 정책을 쓸것임
- 이벤트 -> [S3 버킷] -> 모든 이벤트들 -> [amazon eventBridge에 감] -> 아마존 서비스 18개를 활용할 수 있음 
> 결론 SNS  / sqs / rambda 에서 일어나는 이벤트를 s3에서 amazon eventBridge로 전송해서 그런 이벤트에 반응할 수 있도록 함.


![[스크린샷 2025-05-04 오후 9.38.48.png]]


실습에서 한 것
- s3에 파일을 업로드하면 sqs에서 이벤트를 감지하는 것을 함
- sqs하나 만들고 거기에 정책을 넣어서 누구나 메시지를 보낼 수 있도록 함. 

## Baseline Performance

- S3에서 첫 번째 바이트를 가져오는 데 100~200밀리초의 매우 짧은 지연 시간을 갖습니다. 그래서 이것은 꽤 빠릅니다.
- 그리고 초당 몇 개의 요청을 받을 수 있는지에 관해서는 버킷 내에서 prefix당 초당 3,500개의 넣기(PUT), 복사(COPY), 올리기(POST) 또는 삭제(DELETE)를 하거나 5,500개의 가져오기(GET) 또는 헤드(HEAD) 요청을 가져옵니다 웹사이트에서 이렇게 얻을 수 있게 됩니다
- 버킷의 prefix 수에는 제한이 없습니다
 - 위의 4개 prefix 모두에 균등하게 읽기를 분산하면 헤드 및 가져오기에 대해 초당 22,000개의 요청을 달성할 수 있습니다
- 결론: 정말 고성능이다.



## S3 Performance

S3 성능과 이를 최적화하는 방법

1. 멀티파트 업로드
	- 100MB가 넘는 파일에는 멀티파트 업로드를 사용하는 것이 좋습니다 (= 파일나눠서 올리기)
	- 5GB가 넘는 파일에는 반드시 멀티파트 업로드를 사용해야 합니다
	- 멀티파트 업로드는 업로드를 병렬화하므로 전송 속도를 높여 대역폭을 최대화하는 데 도움이 됩니다
	- 예시
		- 큰 파일을 아마존 S3에 업로드할때, 부분적으로 나눈다 (=멀티파트)
		- 파일의 작은 덩어리 각각이 아마존 S3에 병렬로 업로드됩니다
		- 모든 부분이 업로드되면 이를 큰 파일로 다시 합칠 수 있습니다
2. S3 전송 가속화: 업로드 및 다운로드를 위한 
	- 파일을 AWS 엣지 위치로 전송하여 전송 속도를 높이는 것입니다
	- 그러면 해당 데이터가 대상 지역의 S3 버킷으로 전달됩니다
	- 따라서 엣지 위치는 단순한 지역 이상입니다
	- 현재 200개가 넘는 엣지 위치가 있으며 그 수는 점점 늘어나고 있습니다
	- 그리고 그것이 무엇을 의미하는지 그래프로 보여드리겠습니다
	- 그리고 전송 가속화는 멀티파트 업로드와 호환됩니다
	- 예시
		- 미국에 파일이 있습니다
		- 그리고 이것을 호주에 있는 S3 버킷에 업로드하려고 합니다
		- 따라서 이것이 할 일은 미국의 엣지 위치를 통해 해당 파일을 업로드하는 것입니다
		- 이는 매우 빠르고 공공 인터넷을 사용하게 될 것입니다
		- 그런 다음 해당 엣지 위치에서 호주의 아마존 S3 버킷으로
		- 빠른 프라이빗 AWS 네트워크를 통해 파일을 전송합니다
	- 그래서 이것은 공공 인터넷의 사용량을 최소화하고 프라이빗 AWS 네트워크의 사용량을 최대화하기 때문에 전송 가속화라고 불립니다
	- 전송 가속화는 전송 속도를 높이는 좋은 방법입니다




## S3 byte-range fetches
파일의 특정 바이트 범위를 가져오는 것을 병렬화하는 것
- 특정 바이트 범위를 가져오는 데 실패한 경우에도 더 작은 바이트 범위를 다시 시도할 수 있음.
- 실패 시 복원력이 향상
- 다운로드 속도를 높이는 데 사용할 수 있
- 예시 1: 용량이 매우 큰 파일이 있다고 가정 
	파일의 처음 몇 바이트인 첫 번째 부분을 요청하고 그 다음 두 번째 부분, 그 다음 n번째 부분을 요청하고 싶을 수도 있음 =  이 모든 부분을 구체적으로 요청합니다
	파일의 특정 바이트 범위만 요청하기 때문에 바이트 범위 가져오기라 부름.
- 이 모든 요청은 병렬로 처리될 수 있음
- 결론: 가져오기를 병렬화하고 다운로드 속도를 높일 수 있다는 것입니다

- 예시 2: 파일의 일부만 검색하는 것
	-  **상황**: S3에 저장된 큰 파일이 있는데, 그 중 **처음 50바이트**가 "헤더" 정보라고 정해져 있어요.  
	- **목표**: 전체 파일을 다운로드하지 않고, **이 헤더(처음 50바이트)**만 빠르게 확인하고 싶어요.  
	- **해결법**: S3에 **byte-range fetch 요청**을 해서 **파일의 처음 0~49 바이트**만 요청하면 됩니다.


## S3 Batch Operations 
- **여러 S3 객체에 대해 한 번에 작업을 수행할 수 있는 서비스**
- 사용 사례:    
    - 메타데이터 및 속성(ACL, 태그 등) 대량 수정
    - S3 버킷 간 객체 복사
    - 암호화되지 않은 객체 일괄 암호화
    - S3 Glacier 객체 대량 복원
    - Lambda 함수 호출로 사용자 지정 작업 수행
- 작업 구성:
    - **객체 목록** + **수행할 작업** + **작업 옵션/매개변수**
- 장점:
    - 수동 스크립팅 없이 **재시도, 진행 추적, 알림, 보고서 자동 관리**
- 객체 목록 생성:
    - **S3 Inventory**로 객체 목록 생성
    - **S3 Select**로 원하는 조건의 객체만 필터링
- 대표 예시:
    - S3 Inventory로 암호화되지 않은 객체 찾기 → Batch Operations로 일괄 암호화
        

## S3 Storage Lens 요약

- **AWS 전체 조직의 S3 스토리지 사용을 분석·최적화**하는 서비스
- 주요 기능:
    - **비용 효율성 분석, 이상 징후 탐지, 데이터 보호 상태 평가**
    - **기본 또는 사용자 지정 대시보드** 제공
    - **CSV 또는 Parquet 형식으로 메트릭을 S3로 내보내기 가능**
- 분석 대상:
    - 전체 조직, 계정, 지역, 버킷, 접두사 단위 등
- 메트릭 종류:
    1. **스토리지 메트릭**: 저장 용량, 객체 수 등
    2. **비용 최적화 메트릭**: 불완전한 업로드, 오래된 객체 등
    3. **데이터 보호 메트릭**: 버전 관리, 암호화, MFA 삭제 등 설정 여부
    4. **액세스 관리 메트릭**: 객체 소유권 상태 등
    5. **이벤트 메트릭**: S3 이벤트 알림 구성 여부
    6. **퍼포먼스 메트릭**: S3 전송 가속 활성화 여부 등
    7. **액티비티 메트릭**: 요청 횟수, 다운로드량 등
    8. **HTTP 상태 코드 메트릭**: 200, 403 등의 상태 통계
- **무료 vs 유료**:
    - **무료**: 약 28개 지표 제공, 14일간 데이터 조회 가능
    - **유료**: 활동/비용/보안/상태 코드 등 고급 지표 포함, 15개월 데이터 보존
    - 일부 메트릭은 **CloudWatch와 통합** 가능
       